"""
LangFuse @observe Decorator - AI Response Generator Example

This file demonstrates using the @observe decorator for tracing
AI response generation with provider fallback.
Source: clickbait project - app/services/ai/response/response_generator.py

Key patterns:
- @observe decorator for automatic span creation
- Callbacks propagation to LangChain models
- Token usage tracking
- Latency measurement
- Provider fallback handling
"""

import logging
import time
from typing import Any, TYPE_CHECKING

from langchain_core.callbacks.base import BaseCallbackHandler
from langfuse import observe
from pydantic import BaseModel

if TYPE_CHECKING:
    pass


class AIProviderResponse(BaseModel):
    """Response from an AI provider."""
    content: dict[str, Any]
    input_tokens: int
    output_tokens: int
    total_tokens: int
    model_name: str


class AIResponse(BaseModel):
    """Final AI response with metadata."""
    content: dict[str, Any]
    provider_enum: str
    provider_name: str
    input_tokens: int
    output_tokens: int
    total_tokens: int
    model_name: str


class GenerateResponseConfig(BaseModel):
    """Configuration for response generation."""
    name: str
    prompt: Any
    temperature: float = 0.7
    force_execution: bool = False
    models_to_use: list[str] | None = None
    provider_enums: list[str] | None = None


class MockProvider:
    """Mock AI provider for demonstration."""

    def __init__(self, name: str, provider_enum: str):
        self.name = name
        self.provider_enum = provider_enum

    def generate_response(
        self,
        config: GenerateResponseConfig,
        models_name: list[str] | None = None,
        callbacks: list[BaseCallbackHandler] | None = None,
    ) -> AIProviderResponse:
        # Simulate API call with callbacks for tracing
        model = models_name[0] if models_name else "default-model"

        return AIProviderResponse(
            content={"response": f"Generated content using {model}"},
            input_tokens=100,
            output_tokens=50,
            total_tokens=150,
            model_name=model,
        )


class ResponseGenerator:
    """Generates simple AI responses without agents or tools.

    Handles direct LLM calls with structured/unstructured output,
    provider fallback, and error handling.

    Usage:
        generator = ResponseGenerator()
        response = generator.generate(
            generation_config=config,
            callbacks=[langfuse_handler],
        )
    """

    def __init__(self):
        self.providers = [
            MockProvider("OpenAI", "OPENAI"),
            MockProvider("Anthropic", "ANTHROPIC"),
        ]
        self.current_provider = None
        self.current_provider_enum = None

    @observe(name="response_generator")
    def generate(
        self,
        generation_config: GenerateResponseConfig,
        callbacks: list[BaseCallbackHandler] | None = None,
    ) -> AIResponse:
        """Generate AI response with LangFuse tracing.

        The @observe decorator automatically creates a span for this
        function, capturing inputs, outputs, and timing.

        Args:
            generation_config: Configuration for generation
            callbacks: LangChain callbacks for tracing (includes LangFuse handler)

        Returns:
            AIResponse with content and metadata
        """
        last_exception = None

        for provider in self.providers:
            self.current_provider = provider.name
            self.current_provider_enum = provider.provider_enum

            try:
                # Measure latency
                provider_start_time = time.monotonic()

                # Call provider, passing callbacks for LangChain tracing
                response_data: AIProviderResponse = provider.generate_response(
                    config=generation_config,
                    models_name=generation_config.models_to_use,
                    callbacks=callbacks,
                )

                provider_latency_ms = int(
                    (time.monotonic() - provider_start_time) * 1000
                )

                logging.info(
                    f"LLM valid response generated by provider {provider.name} - "
                    f"{response_data.model_name} - Latency: {provider_latency_ms}ms",
                )

                input_tokens = response_data.input_tokens
                output_tokens = response_data.output_tokens
                total_tokens = response_data.total_tokens

                logging.info(
                    f"Total tokens: {total_tokens} - Input: {input_tokens} | Output: {output_tokens}"
                )

                return AIResponse(
                    content=response_data.content,
                    provider_enum=provider.provider_enum,
                    provider_name=provider.name,
                    input_tokens=input_tokens,
                    output_tokens=output_tokens,
                    total_tokens=total_tokens,
                    model_name=response_data.model_name,
                )

            except Exception as error:
                logging.error(f"Provider {provider.name} failed: {error}")
                last_exception = error
                continue

        raise Exception(
            f"All providers failed to generate response"
        ) from last_exception


# ============================================================================
# USAGE EXAMPLE
# ============================================================================

if __name__ == "__main__":
    import os
    from langfuse import get_client
    from langfuse.langchain import CallbackHandler

    os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-test"
    os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-test"

    langfuse = get_client()
    langfuse.update_current_trace(
        name="response-generation-example",
        tags=["example", "demo"],
    )

    langfuse_handler = CallbackHandler()

    generator = ResponseGenerator()
    config = GenerateResponseConfig(
        name="example_generation",
        prompt=None,
        models_to_use=["gpt-4"],
    )

    response = generator.generate(
        generation_config=config,
        callbacks=[langfuse_handler],
    )

    print(f"Response: {response.content}")
    print(f"Tokens: {response.total_tokens} (in: {response.input_tokens}, out: {response.output_tokens})")
    print(f"Provider: {response.provider_name}")
    print(f"Model: {response.model_name}")

    get_client().flush()
